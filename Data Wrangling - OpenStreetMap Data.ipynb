{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "\n",
    "### Map Area\n",
    "Southampton, England\n",
    "\n",
    "- [Southampton Map Data](https://mapzen.com/data/metro-extracts/metro/southampton_england/)\n",
    "\n",
    "\n",
    "In the 2001 census Southampton and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become the sixth-largest built-up area in England with a population of 855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is also known as Solent City, particularly in the media when discussing local governance organisational changes. With a population of over 1.5 million this makes the region one of the United Kingdom's most populous metropolitan areas.\n",
    "\n",
    "# Project Overview\n",
    "To choose any area from the world map provided on [OpenStreetmap](https://www.openstreetmap.org) and use data wrangling techniques, such as assessing the quality of the data for validity,accuracy,completeness, consistency and uniformity, to clean the OpenStreeMap Data for that part of the world and finally, use the SQL as the data schema to complete the project by sorting, querying and aggregating the data.\n",
    "\n",
    "\n",
    "## Problems Encountered in the Map\n",
    "After initially downloading a small sample size of the Southamptonn area and running it against a provisional code, I noticed five main problems with the data, which I will discuss in the following order:\n",
    "\n",
    "\n",
    "- Wrongly­abbreviated street names *(“Bellevue Rd”)*\n",
    "\n",
    "# OpenStreetMap\n",
    "OpenStreetMap is a community built free editable map of the world, inspired by the success of Wikipedia where crowdsourced data is open and free from proprietary restricted use. We see some examples of its use by Craigslist and Foursquare, as an open source alternative to Google Maps.\n",
    "http://www.openstreetmap.org<br>\n",
    "Users can map things such as polylines of roads, draw polygons of buildings or areas of interest, or insert nodes for landmarks. These map elements can be further tagged with details such as street addresses or amenity type. Map data is stored in an XML format. More details about the OSM XML can be found here:<br>\n",
    "http://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "Some highlights of the OSM XML format relevent to this project are:<br>\n",
    "<ul>\n",
    "OSM XML is list of instances of data primatives (nodes, ways, and relations) found within a given bounds\n",
    "<li>***nodes*** represent dimensionless points on the map</li>\n",
    "<li>***ways*** contain node references to form either a polyline or polygon on the map</li>\n",
    "<li>nodes and ways both contain children tag elements that represent key value pairs of descriptive information about a given node or way</li>\n",
    "As with any user generated content, there is likely going to be dirty data. In this project I'll attempt to do some auditing, cleaning, and data summarizing tasks with Python and MongoDB.<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the OSM XML file downloaded, lets parse through it with ElementTree and count the number of unique element types. Iterative parsing is utilized since the XML is too large to process in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"southampton_england.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our file southampton_england.osm is 64.94942855834961 MB or 0.06342717632651329 GB\n"
     ]
    }
   ],
   "source": [
    "print (\"Size of our file {} is {} MB or {} GB\".format(filename,os.path.getsize(filename)/(1024.0*1024), \\\n",
    "                                                       os.path.getsize(filename)/(1024.0*1024*1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible we have used the dataset having a size of 65 MB or 0.06 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 12189,\n",
      " 'nd': 382436,\n",
      " 'node': 277424,\n",
      " 'osm': 1,\n",
      " 'relation': 1194,\n",
      " 'tag': 208301,\n",
      " 'way': 52405}\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "for event, elem in ET.iterparse(filename):\n",
    "    if elem.tag not in tag_dict:\n",
    "        tag_dict[elem.tag] = 1\n",
    "    else:\n",
    "        tag_dict[elem.tag] += 1\n",
    "\n",
    "# for key, value in sorted(tag_dict.items(), key=lambda key,val: val,key):\n",
    "#     print (\"%s: %s\") % (key, value)\n",
    "pprint.pprint(tag_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after processing we could see the distinct tags that are available for us are as follows:<br>\n",
    "<ul>\n",
    "<li>bounds: 1</li>\n",
    "<li>osm: 1</li>\n",
    "<li>relation: 1194</li>\n",
    "<li>member: 12189</li>\n",
    "<li>way: 52405</li>\n",
    "<li>tag: 208301</li>\n",
    "<li>node: 277424</li>\n",
    "<li>nd: 382436</li>\n",
    "</ul>\n",
    "\n",
    "## Basic idea about what these tags want to convey to us\n",
    "\n",
    "We are ultimately interested in the following three tags:\n",
    "<ol>\n",
    "<li>**node** (represented by tag ```<node></node>```)- which consist of \"a single point in space defined by its latitude, longitude and node id. Nodes can be used to define standalone point features. In this case, a node will normally have at least one tag to define its purpose. Nodes are often used to define the shape or \"path\" of a way.\"</li>\n",
    "<li>**relation** (represented by tag ```<relation></relation>```)- which are \"used to model logical (and usually local) or geographic relationships between objects\". To make it simple, these are areas made of several ways.</li>\n",
    "<li>**ways** (represented by tag ```<way></way>```)- which are an \"ordered list of nodes and normally also has at least one tag\". To make it simple, these are streets, avenues, etc.</li>\n",
    "</ol>\n",
    "\n",
    "```<nd> tags``` references the id of the node as an integer. These are the tags which represent the **Nodes** that make up the **ways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!! After having a look at the length and breadth of our data, we can now explore it to answer some amazing questions :) <br><br> Let's first answer one question\n",
    "\n",
    "# How many users have made the map of Southampton City Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n"
     ]
    }
   ],
   "source": [
    "def get_users(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        for att in element.attrib:\n",
    "            if att == 'uid':\n",
    "                if element.attrib['uid'] not in users:\n",
    "                    users.add(element.attrib['uid'])\n",
    "    return users\n",
    "\n",
    "print(len(get_users(filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its great to see that 541 users contributed towards making the city of Southampton's map better...\n",
    "Lets see what they have contributed towards the map. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audit tag key string types\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "lo = set()\n",
    "lo_co = set()\n",
    "pro_co = set()\n",
    "oth = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we categorize our data according to its contents.\n",
    "We use `Regular Expressions (Regex)` for this.\n",
    "- The tags having only the lower characters are categorized as **lower**\n",
    "- The tags having the lower characters with a colon are categorized as **lower_colon**\n",
    "- **problemchars** are those having unexpected characters\n",
    "- **others** are the ones which do not fall in the above mentioned catagories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following Function utilises the power of regular expressions.<br>\n",
    "Here we categorise our data according to the regular expressions mentioned above.<br>\n",
    "After the categorisation, we return the counts of our data in the form of a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 113601, 'lower_colon': 92129, 'other': 2569, 'problemchars': 2}\n"
     ]
    }
   ],
   "source": [
    "def key_type(element, keys): \n",
    "    # Here we are actually segregating our data according to the its type!\n",
    "    if element.tag == \"tag\":\n",
    "        low = lower.search(element.attrib['k']) # Data \n",
    "        low_col = lower_colon.search(element.attrib['k'])\n",
    "        prob = problemchars.search(element.attrib['k'])\n",
    "        if low:\n",
    "            keys[\"lower\"] += 1\n",
    "            lo.add(element.attrib['k']) \n",
    "        elif low_col:\n",
    "            keys[\"lower_colon\"] +=1\n",
    "            lo_co.add(element.attrib['k']) \n",
    "        elif prob:\n",
    "            keys[\"problemchars\"] +=1\n",
    "            pro_co.add(element.attrib['k']) \n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "            oth.add(element.attrib['k'])\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "#     keys = {\"lower\": len(lo), \"lower_colon\": len(lo_co), \"problemchars\": len(pro_co), \"other\": len(oth)}            \n",
    "    return keys\n",
    "\n",
    "keys = process_map(filename)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the **postal codes of Southampton City**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO18 2HW\n",
      "SO17 1TW\n",
      "SO16\n",
      "SO17\n",
      "SO17\n",
      "SO17\n",
      "SO14\n",
      "SO14\n",
      "SO14\n",
      "SO14\n",
      "SO14\n",
      "SO14\n",
      "SO17\n",
      "SO14\n",
      "SO17\n",
      "SO14\n",
      "SO15\n",
      "SO15\n",
      "SO16\n",
      "SO16\n",
      "SO16\n",
      "SO15\n",
      "SO17\n",
      "SO19 7QN\n",
      "SO15 5NF\n",
      "SO17 1BJ\n",
      "SO17 3SP\n",
      "SO16 3HP\n",
      "SO16 3FH\n",
      "SO15 4GW\n",
      "SO19 9PE\n",
      "SO19 9PE\n",
      "SO19 9PE\n",
      "SO19 9PE\n",
      "SO15 1BA\n",
      "SO19 9PE\n",
      "SO18 1LN\n",
      "SO30 3DT\n",
      "SO19 9PE\n",
      "SO19 9PE\n",
      "SO15 4GW\n",
      "SO19 9PE\n",
      "SO18 2LB\n",
      "SO15 0NB\n"
     ]
    }
   ],
   "source": [
    "def print_postal_codes(element):\n",
    "    if element.tag == \"tag\" :\n",
    "        if element.attrib['k']==\"postal_code\":\n",
    "            print (element.attrib['v'])\n",
    "for _, element in ET.iterparse(filename):\n",
    "    print_postal_codes(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above codes are synonymous with the actual postal codes of southampton city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check for our data to be consistent.<br>\n",
    "**Formerly we want to check if our street names are abbreviated properly**\n",
    "<br>Inconsistent data would be handled later on here we just check for our entries which are not desirable<br>\n",
    "**expected** contains the list of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Rd': {'Bellevue Rd', 'Grange Rd', 'Hythe Rd'},\n",
       "             're': {'Royal Crescent Road student re'},\n",
       "             'Westal': {'Bitterne Road Westal'},\n",
       "             'Raod': {'Bluebell Raod'},\n",
       "             'road': {'Bluebell road', 'bluebell road'},\n",
       "             'Roads': {'Octavia Roads'}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(set)\n",
    "\n",
    "expected = [\"Court\",\"Polygon\",\"East\",\"West\",\"North\",\"South\",\"Road\",\"Avenue\",\"Hill\",\"Terrace\"\\\n",
    "            ,\"Greenways\",\"Park\",\"Mews\",\"Way\",\"Lane\",\"Gree\",\"Grove\",\"Drive\",\"Mount\",\"Place\",\\\n",
    "            \"Broadway\",\"Gardens\",\"Square\",\"Walk\",\"Street\",\"Crescent\",\"Close\",\"Estate\",\"Walk\",\\\n",
    "            \"Green\",\"Saltmead\",\"Square\",\"Buildings\",\"House\",\"View\",\"Meadow\",\"View\",\"Village\"\\\n",
    "            ,\"Bridge\",\"Centre\",\"Holt\",\"Mayflowers\",\"Redhill\",\"Meadow\",\"Quay\",\"Esplanade\",\\\n",
    "            \"Cottages\",\"Finches\",\"Parade\",\"Dell\",\"Rise\",\"Cloisters\",\"Drove\",\"Loop\",\"access\",\\\n",
    "            \"Street)\",\"High-Rise\",\"Queensway\",\"S\",\"Firs\",\"Precinct\",\"387\"]\n",
    "\n",
    "#expected is a list of all the names of the street which are consistent according to the city.\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "#     for key, value in sorted(street_types.iteritems(), key=lambda (k,v): (v,k)):\n",
    "#         print \"%s: %s\" % (key, value)\n",
    "\n",
    "    return(street_types)\n",
    "\n",
    "all_types = audit(filename)\n",
    "all_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from our above printed data we could figure out those few entries which we are looking for updating and would make up our data seterror free `:)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Clean our Data\n",
    "> # Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { \"Raod\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"road\" : \"Road\",\n",
    "            \"Roads\" : \"Road\",\n",
    "           \"re\": \"Residence\",\n",
    "           \"Westal\":\"West\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapping** - is the dictionary we made to correct our streetnames which were wrongly entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_name function fixes our **streetnames** as mentioned in our **mapping** dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    street_type = name.rsplit(' ')[-1]\n",
    "    m = street_type_re.search(name)\n",
    "    street_name = name.rsplit(' ' , 1)[0]\n",
    "    if street_type in mapping:\n",
    "        name = street_name + ' ' + mapping[street_type]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we traverse through all the entries of our street_types and fix them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellevue Rd => Bellevue Road\n",
      "Grange Rd => Grange Road\n",
      "Hythe Rd => Hythe Road\n",
      "Royal Crescent Road student re => Royal Crescent Road student Residence\n",
      "Bitterne Road Westal => Bitterne Road West\n",
      "Bluebell Raod => Bluebell Road\n",
      "Bluebell road => Bluebell Road\n",
      "bluebell road => bluebell Road\n",
      "Octavia Roads => Octavia Road\n"
     ]
    }
   ],
   "source": [
    "for street_type, ways in street_types.items():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "#         for event, elem in ET.iterparse(filename):\n",
    "#             if is_street_name(elem):\n",
    "#                 if elem.attrib['v'] == name:\n",
    "#                     update(elem.attrib['v'],better_name)\n",
    "        print (name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing our data for inserting in SQL :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so we will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "To make this process easier we've already defined a schema (in the schema.py file) for the .csv files and the eventual tables. Using the cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". \n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply import our much needed packages which would facilitate us to process our data and create CSV files for insertion in our SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cerberus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = filename\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load our data into SQL from CSV we need to make sure that it should be properly structured according to the tags and fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def process_key(key_string): \n",
    "      \"\"\"\n",
    "      This function processes 'k' values to slice and separate key strings into\n",
    "      their respective keys and tag types. It returns an ordered listed with\n",
    "      the new key and the tag type. \n",
    "      \"\"\"\n",
    "      if \":\" in key_string:   \n",
    "            indexed_string = key_string.find(\":\")\n",
    "            tag_type = key_string[:indexed_string]\n",
    "            new_key = key_string[indexed_string+1:]\n",
    "            return [new_key, tag_type]\n",
    "      else:\n",
    "            new_key = key_string\n",
    "            tag_type = \"regular\"\n",
    "            return [new_key, tag_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally in our shape_element function, we structure the data according to the tag it comes from..\n",
    "<br>Node data is collected in one schema and<br>\n",
    "Way data is collected in one schema :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "\n",
    "      # first loop through to get node's attributes and values into a dictinonary\n",
    "        for attrName, attrValue in element.attrib.items():\n",
    "            if attrName in NODE_FIELDS:\n",
    "                node_attribs[attrName] = attrValue\n",
    "        #print node_attribs\n",
    "\n",
    "        \"\"\" \n",
    "        Next, loop through the child tags and parse out the\n",
    "        key, value, and clean up the 'key' to create types. Then\n",
    "        put everything into a dictionary to append to tags list.\n",
    "        \"\"\"\n",
    "        for i in element.iter('tag'):\n",
    "            #print i\n",
    "            temp_dict = {}\n",
    "            if PROBLEMCHARS.search(i.attrib['k']):\n",
    "                continue\n",
    "            else:\n",
    "                temp_dict['id'] = element.attrib['id']\n",
    "                temp_dict['key'] = process_key(i.attrib['k'])[0]\n",
    "                temp_dict['type'] = process_key(i.attrib['k'])[1]\n",
    "                temp_dict['value'] = update_name(i.attrib['v'],mapping)\n",
    "                #print temp_dict\n",
    "            tags.append(temp_dict)\n",
    "        #print tags\n",
    "\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "\n",
    "        for attrName, attrValue in element.attrib.items():\n",
    "            if attrName in WAY_FIELDS:\n",
    "                #print attrName\n",
    "                #print attrValue\n",
    "                way_attribs[attrName] = attrValue\n",
    "        #print way_attribs\n",
    "\n",
    "        \"\"\" \n",
    "        Since the way tags follow the same rules as the node tags, these\n",
    "        are processed the same way.\n",
    "        \"\"\"\n",
    "        for i in element.iter('tag'):\n",
    "            temp_dict = {}\n",
    "            if PROBLEMCHARS.search(i.attrib['k']):\n",
    "                continue\n",
    "            else:\n",
    "                temp_dict['id'] = element.attrib['id']\n",
    "                temp_dict['key'] = process_key(i.attrib['k'])[0]\n",
    "                temp_dict['type'] = process_key(i.attrib['k'])[1]\n",
    "                temp_dict['value'] = update_name(i.attrib['v'],mapping)\n",
    "            tags.append(temp_dict)\n",
    "#         print (tags)\n",
    "\n",
    "        \"\"\"\n",
    "        enumerate() is used here to create a counter for each 'nd' child node.\n",
    "        \"\"\"\n",
    "\n",
    "        for counter, i in enumerate(element.iter('nd')):\n",
    "            temp_dict = {}\n",
    "            temp_dict['id'] = element.attrib['id']\n",
    "            temp_dict['node_id'] = i.attrib['ref']\n",
    "            temp_dict['position'] = counter\n",
    "            way_nodes.append(temp_dict)\n",
    "        #print (way_nodes)\n",
    "\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions help the main functions to get smaller tasks done `:)` like getting, validating elements and finally writing the data into CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.items())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the BOSS, the main function which would help us driving the process according to our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of helper functions and the main function, our data is used to create the CSV files.\n",
    "The CSV files created have the following sizes :\n",
    "    - nodes.csv - 24 MB\n",
    "    - nodes_tags.csv - 1 MB\n",
    "    - ways.csv - 3 MB\n",
    "    - ways_nodes.csv - 9.1 MB\n",
    "    - ways_tags.csv - 6.2 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "# sample of the map when validating.\n",
    "# from time import time\n",
    "# t0 = time()\n",
    "process_map(OSM_PATH, validate=False)\n",
    "# time_taken = time-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's gear up for accessing our data through SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tables for the SQL database\n",
    "nodes = '''CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");'''\n",
    "\n",
    "nodes_tags = '''CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");'''\n",
    "\n",
    "ways = '''CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");'''\n",
    "\n",
    "ways_tags = '''CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");'''\n",
    "\n",
    "ways_nodes = '''CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would create and access our Database file - southamptonDW.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('southamptonDW.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table nodes already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-56f4ca2e6b0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mways\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mways_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: table nodes already exists"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(nodes)\n",
    "cursor.execute(nodes_tags)\n",
    "cursor.execute(ways)\n",
    "cursor.execute(ways_tags)\n",
    "cursor.execute(ways_nodes)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using our commit function.... all the tables get created in our database inside SQL\n",
    "Now its our task to populate all these tables with the data extracted from the given Dataset (OSM)\n",
    "> # Let's use our CSV's which are ready for insertion in SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-037a0afb1e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nodes_tags.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mto_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_db\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-037a0afb1e96>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nodes_tags.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mto_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_db\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# Populate the database tables with data from CSVs\n",
    "# Read in the csv file as a dictionary and formatting the data as a list of tuples,\n",
    "# then insert the formatted data in the database table \n",
    "\n",
    "# populating the nodes_tags table\n",
    "with open('nodes_tags.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type']) for i in dr]\n",
    "    \n",
    "cursor.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "# populating the ways table\n",
    "with open('ways.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['user'].decode(\"utf-8\"), i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cursor.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "# populating the nodes table\n",
    "with open('nodes.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'].decode(\"utf-8\"), i['uid'], i['version'], i['changeset'], i['timestamp']) \n",
    "             for i in dr]\n",
    "\n",
    "cursor.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", \n",
    "                   to_db)\n",
    "conn.commit()\n",
    "\n",
    "# populating the ways_tags table\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type']) for i in dr]\n",
    "    \n",
    "cursor.executemany(\"INSERT INTO ways_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "# populating the ways_nodes table \n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "    \n",
    "cursor.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the queries are executed successfully our database size becomes 40 MB\n",
    "\n",
    "> Our Data Files have sizes as follows :\n",
    "- Southampton_englang.osm - 69 MB\n",
    "- southamptonDW.db - 39 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the sql database\n",
    "# Establish connection and cursor\n",
    "conn = sqlite3.connect('southamptonDW.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(132707,\n",
      "  50.9454657,\n",
      "  -1.4775675,\n",
      "  'monxton',\n",
      "  260682,\n",
      "  5,\n",
      "  8139974,\n",
      "  '2011-05-14T11:45:29Z'),\n",
      " (132708,\n",
      "  50.9474216,\n",
      "  -1.4709162,\n",
      "  'Deanna Earley',\n",
      "  2231,\n",
      "  1,\n",
      "  153019,\n",
      "  '2006-11-11T17:58:16Z'),\n",
      " (132709,\n",
      "  50.9507933,\n",
      "  -1.4643494,\n",
      "  'Deanna Earley',\n",
      "  2231,\n",
      "  1,\n",
      "  153019,\n",
      "  '2006-11-11T17:21:36Z'),\n",
      " (132710,\n",
      "  50.9533581,\n",
      "  -1.4594265,\n",
      "  'Deanna Earley',\n",
      "  2231,\n",
      "  1,\n",
      "  153019,\n",
      "  '2006-11-11T17:24:45Z'),\n",
      " (170434,\n",
      "  50.9344749,\n",
      "  -1.3283489,\n",
      "  'Nick Austin',\n",
      "  14020,\n",
      "  3,\n",
      "  11109218,\n",
      "  '2012-03-26T19:10:22Z')]\n"
     ]
    }
   ],
   "source": [
    "# Let's check first 5 entries in the nodes table.\n",
    "query = '''\n",
    "select * from nodes limit 5'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(86,)]\n"
     ]
    }
   ],
   "source": [
    "# Let's count the Number of restaurants in Southampton\n",
    "query = '''SELECT COUNT (*) FROM nodes_tags WHERE key == \"amenity\" and value == \"restaurant\";'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(534393, 'Chris Baines', 106867),\n",
      " (1569426, 'Harjit (CabMyRide)', 24901),\n",
      " (55782, '0123456789', 23548),\n",
      " (14020, 'Nick Austin', 17434),\n",
      " (84000, 'pcman1985', 13983),\n",
      " (2231, 'Deanna Earley', 13213),\n",
      " (1540938, 'Arjan Sahota', 12907),\n",
      " (972618, 'Kuldip (CabMyRide)', 9572),\n",
      " (2098, 'Andy Street', 9079),\n",
      " (670691, 'Harry Cutts', 6668)]\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT uid, user, COUNT(*) as num from (SELECT uid, user FROM nodes UNION ALL SELECT uid, user from ways) as united \n",
    "GROUP BY uid ORDER BY num DESC LIMIT 10;'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(238172,)]\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of “ways” and “nodes” attributable to the top 10 users\n",
    "query = '''SELECT sum(num) FROM (SELECT uid, COUNT(*) as num from (SELECT uid FROM nodes UNION ALL SELECT uid from ways) \n",
    "as united GROUP BY uid ORDER BY num DESC LIMIT 10) as topsomany;'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('atm',),\n",
      " ('bank',),\n",
      " ('bar',),\n",
      " ('bbq',),\n",
      " ('bench',),\n",
      " ('bicycle_parking',),\n",
      " ('bicycle_rental',),\n",
      " ('bicycle_repair_station',),\n",
      " ('billboard',),\n",
      " ('cafe',),\n",
      " ('car_rental',),\n",
      " ('car_sharing',),\n",
      " ('car_wash',),\n",
      " ('charging_station',),\n",
      " ('cinema',),\n",
      " ('clock',),\n",
      " ('community_centre',),\n",
      " ('dentist',),\n",
      " ('doctors',),\n",
      " ('drinking_water',),\n",
      " ('fast_food',),\n",
      " ('ferry_terminal',),\n",
      " ('fire_station',),\n",
      " ('fountain',),\n",
      " ('fuel',),\n",
      " ('hospital',),\n",
      " ('kindergarten',),\n",
      " ('library',),\n",
      " ('marketplace',),\n",
      " ('motorcycle_parking',),\n",
      " ('nightclub',),\n",
      " ('parking',),\n",
      " ('parking_entrance',),\n",
      " ('parking_space',),\n",
      " ('pharmacy',),\n",
      " ('place_of_worship',),\n",
      " ('police',),\n",
      " ('post_box',),\n",
      " ('post_depot',),\n",
      " ('post_office',),\n",
      " ('pub',),\n",
      " ('recycling',),\n",
      " ('restaurant',),\n",
      " ('sauna',),\n",
      " ('school',),\n",
      " ('shelter',),\n",
      " ('social_facility',),\n",
      " ('swimming_pool',),\n",
      " ('taxi',),\n",
      " ('telephone',),\n",
      " ('toilets',),\n",
      " ('training',),\n",
      " ('vending_machine',),\n",
      " ('veterinary',),\n",
      " ('waste_basket',)]\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT DISTINCT value FROM (SELECT value FROM nodes_tags WHERE key == \"amenity\") \n",
    "            as allamenities ORDER BY value;'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tower',), ('sub_station',), ('generator',), ('pole',), ('substation',)]\n"
     ]
    }
   ],
   "source": [
    "# Lets check the power distribution methods\n",
    "query = '''SELECT DISTINCT value FROM nodes_tags WHERE key == 'power';'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('indian', 10),\n",
      " ('italian', 8),\n",
      " ('chinese', 8),\n",
      " ('thai', 4),\n",
      " ('pizza', 4),\n",
      " ('sushi', 2),\n",
      " ('spanish', 2),\n",
      " ('scandinavian', 2),\n",
      " ('greek', 2),\n",
      " ('chicken', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = '''SELECT value, count(*) as num FROM nodes_tags WHERE key == \"cuisine\" AND id IN \\\n",
    "(SELECT id FROM nodes_tags WHERE key == \"amenity\" and value == \"restaurant\") GROUP BY value ORDER BY num DESC limit 10;'''\n",
    "\n",
    "cursor.execute(query)\n",
    "all_rows = cursor.fetchall()\n",
    "pprint.pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Other ideas about the dataset!\n",
    "\n",
    "### Benefits:\n",
    "> So Far we could realise few things from our dataset, but not limited to the following :\n",
    "- OpenStreetMap project gives us a great opportunity to explore the data open source way, with a greater degree of freedom to download, analyse and utilise the information.\n",
    "- It has a wide variety of information and is ready to accept different type of attributes like cuisines,school,college,streets etc. ready to be downloaded in the format of OSM\n",
    "- Using the XML we can gather greater insights like :\n",
    "* How many restaurants are there?\n",
    "* How many restaurants offer a particular cuisine like Indian, Italian, etc.\n",
    "\n",
    "### Anticipated Problems \n",
    "> From exploring the OpenStreetMap dataset, I found the data structure to be flexible enough to include a vast multitude of user generated quantitative and qualitative data beyond that of simply defining a virtual map. There's plenty of potential to extend OpenStreetMap to include user reviews of establishments, subjective areas of what classifies a good vs bad neighborhood, housing price data, school reviews, walkability/bikeability, quality of mass transit, and a bunch of other metrics that could form a solid foundation for robust recommender systems. These recommender systems could aid users in deciding where to live or what cool food joints to check out.\n",
    "\n",
    "> Since the OpenStreetMap project depends upon the users like us to update itself and improve itself, I would say it is certainly providing us good data but I am sure it will keep up upgrading itself!\n",
    "\n",
    "- It would be  a great project as to involve governmental agencies to constantly improve the database of the OpenStreetMap and spread awareness about such a great project that is available for public use Open Source.\n",
    "- Since, OpenStreetMap depends upon donations and user contributions it still has many opportunities for including larger areas of the neighbourhood into it.\n",
    "- Although the OpenStreetMap Project for the city of Southampton is of reasonable quality when it comes to basic typography, still the Southampton neighbourhood remains incomplete. The data still needs more information regarding the amenities, tourist locations, and other points of interest.\n",
    "> Already today, the level of detail is incomparable to that of Google Maps, this gives me greater hope for the project and shows us the importance of collaborative mapping in the areas of the world ignored by major mapping companies.\n",
    "\n",
    "# Conclusion\n",
    "So far we have implemented the Data Wrangling principles like Auditing, Cleaning data and then used it through SQL Database systems.  \n",
    "\n",
    "After using the data extraction through SQL, as stated in the tutorials it is much faster to get data processed from database systems like SQL than to manually search through python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completing this project I also referred to the following links\n",
    "- [1] : https://github.com/bestkao/data-wrangling-with-openstreetmap-and-mongodb/blob/master/data-wrangling-with-openstreetmaps.ipynb\n",
    "        \n",
    "- [2] : https://github.com/georgenizharadze/OpenStreetMap-data-wrangling-and-SQL/blob/master/Project_Kyiv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
